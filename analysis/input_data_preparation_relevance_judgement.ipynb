{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd589ad0",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e907209b",
   "metadata": {},
   "source": [
    "## Converting Summaries to input.jsonl\n",
    "This section creates a new JSONL in the same format as `inputs.jsonl` (fields: `query_id`, `doc_id`, `query`, `text`). It preserves `query_id`, `doc_id`, and `query` from the original inputs and replaces `text` with the summary from the summaries JSONL, matched on `doc_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3f507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count before: 11386, after merge: 11386\n",
      "Summaries matched: 11386 / 11386; missing: 0\n",
      "Wrote: ../data/msmarco-passage-trec-dl-2020-judged/inputs_from_summaries_dl-2020_80tokens.jsonl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths (edit if needed)\n",
    "dataset_name = 'dl-2020'\n",
    "tokens_file = '80tokens'\n",
    "inputs_path = Path(f'../data/msmarco-passage-trec-{dataset_name}-judged/inputs.jsonl')\n",
    "summaries_path = Path(f'summarisation_outputs/gpt_summaries_{dataset_name}_gpt-4o_{tokens_file}.jsonl')\n",
    "output_path = Path(f'../data/msmarco-passage-trec-{dataset_name}-judged/inputs_from_summaries_{dataset_name}_{tokens_file}.jsonl')\n",
    "\n",
    "# Load inputs.jsonl\n",
    "inputs = pd.read_json(inputs_path, lines=True, dtype={'doc_id': str, 'query_id': str})\n",
    "inputs['doc_id'] = inputs['doc_id'].astype(str)\n",
    "\n",
    "# Load summaries JSONL (expects doc_id and summarisation_result)\n",
    "sums = pd.read_json(summaries_path, lines=True, dtype={'doc_id': str})\n",
    "sums['doc_id'] = sums['doc_id'].astype(str)\n",
    "\n",
    "# Pick the text source column from summaries\n",
    "text_col = \"summarisation_result\"\n",
    "\n",
    "# Reduce summaries to a single row per doc_id to avoid many-to-many expansion\n",
    "# Prefer non-null values and keep the longest summary when duplicates exist.\n",
    "sums_clean = (\n",
    "    sums.dropna(subset=[text_col])\n",
    "        .assign(_len=sums[text_col].astype(str).str.len())\n",
    "        .sort_values(['doc_id', '_len'])\n",
    "        .drop_duplicates(subset=['doc_id'], keep='last')\n",
    "        .drop(columns=['_len'])\n",
    "        [['doc_id', text_col]]\n",
    ")\n",
    "\n",
    "# Perform a validated left merge (inputs may have many rows per doc_id, summaries must be one)\n",
    "try:\n",
    "    merged = inputs.merge(sums_clean, on='doc_id', how='left', validate='many_to_one')\n",
    "except Exception as e:\n",
    "    print(f\"Warning: many-to-one validation failed ({e}); proceeding with de-duplicated merge.\")\n",
    "    merged = inputs.merge(sums_clean, on='doc_id', how='left')\n",
    "\n",
    "# Sanity check: row count should not increase\n",
    "print(f\"Row count before: {len(inputs)}, after merge: {len(merged)}\")\n",
    "\n",
    "missing = merged[text_col].isna().sum()\n",
    "print(f\"Summaries matched: {len(merged) - missing} / {len(merged)}; missing: {missing}\")\n",
    "\n",
    "# Build records in inputs format\n",
    "records_df = merged.assign(text_summary=merged[text_col])[['query_id','doc_id','query','text_summary','text']]\n",
    "\n",
    "# Optional: keep only rows with a summary\n",
    "# records_df = records_df.dropna(subset=['text_summary'])\n",
    "\n",
    "records = records_df.to_dict(orient='records')\n",
    "\n",
    "# Write JSONL\n",
    "with output_path.open('w', encoding='utf-8') as f:\n",
    "    for rec in records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False))\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Wrote: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
